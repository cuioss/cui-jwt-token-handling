name: JMH Benchmark

on:
  pull_request:
    branches: [ "main" ]
    types: [ closed ]
  push:
    tags: [ "*" ]
  workflow_dispatch:

# Declare default permissions as read only
permissions: read-all

# Prevent concurrent benchmark runs to avoid interference
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false  # Don't cancel in-progress runs as benchmarks are expensive

jobs:
  benchmark:
    name: Run JMH Benchmarks
    runs-on: ubuntu-latest
    # Only run on merged PRs, not just closed ones
    if: github.event_name != 'pull_request' || github.event.pull_request.merged == true
    # Add timeout to prevent long-running jobs (increased for integration benchmarks)
    timeout-minutes: 45
    permissions:
      # Needed to upload artifacts
      contents: write

    steps:
      - name: Harden the runner (Audit all outbound calls)
        uses: step-security/harden-runner@002fdce3c6a235733a90a27c80493a3241e56863 # v2.12.1
        with:
          egress-policy: audit

      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          fetch-depth: 0  # Fetch all history for proper versioning

      - name: Set up JDK 21
        uses: actions/setup-java@c5195efecf7bdfc987ee8bae7a71cb8b11521c00 # v4.7.1
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: maven

      - name: Build cui-jwt-validation
        run: |
          # Build validation module first to ensure test artifact is available for benchmarking
          ./mvnw --no-transfer-progress clean install -DskipTests

      - name: Run Micro Benchmarks
        run: |
          # Create directory for benchmark results
          mkdir -p benchmark-results

          # Run micro benchmarks with JSON output format, skipping tests to avoid duplicate runs
          # Configure JMH parameters for CI environment: fewer iterations for faster execution
          ./mvnw --no-transfer-progress clean verify -pl cui-jwt-benchmarking -Dskip.benchmark=false -DskipTests \
            -Djmh.result.format=JSON \
            -Djmh.result.filePrefix=benchmark-results/micro-benchmark-result \
            -Djmh.iterations=3 \
            -Djmh.warmupIterations=2 \
            -Djmh.forks=1 \
            -Djmh.threads=2

          # Move micro benchmark results to expected location for upload
          if [ -d "cui-jwt-benchmarking/benchmark-results" ]; then
            echo "Moving micro benchmark results to upload location..."
            mv cui-jwt-benchmarking/benchmark-results/* benchmark-results/ 2>/dev/null || true
          fi

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435 # v3.7.1

      - name: Run Integration Benchmarks
        run: |
          # Build native image first for integration tests
          echo "ðŸ”¨ Building native image for integration tests..."
          ./mvnw --no-transfer-progress clean package -Pintegration-tests -pl cui-jwt-quarkus-parent/cui-jwt-quarkus-integration-tests -DskipTests
          
          # Run integration benchmarks with native image
          echo "ðŸš€ Running integration benchmarks with native Quarkus..."
          ./mvnw --no-transfer-progress clean verify -pl cui-jwt-quarkus-parent/quarkus-integration-benchmark -Dskip.benchmark=false -DskipTests \
            -Djmh.result.format=JSON \
            -Djmh.result.filePrefix=benchmark-results/integration-benchmark-result \
            -Djmh.iterations=3 \
            -Djmh.warmupIterations=2 \
            -Djmh.forks=1 \
            -Djmh.threads=2

          # Move integration benchmark results to expected location for upload
          if [ -d "cui-jwt-quarkus-parent/quarkus-integration-benchmark/benchmark-results" ]; then
            echo "Moving integration benchmark results to upload location..."
            mv cui-jwt-quarkus-parent/quarkus-integration-benchmark/benchmark-results/* benchmark-results/ 2>/dev/null || true
          fi

          # Add timestamp to results
          echo "{ \"timestamp\": \"$(date -u +"%Y-%m-%dT%H:%M:%SZ")\", \"commit\": \"${{ github.sha }}\" }" > benchmark-results/metadata.json

          # Verify that benchmark results were generated
          echo "Checking for benchmark results..."
          ls -la benchmark-results/
          find . -name "*benchmark-result*.json" -type f || echo "No benchmark result files found anywhere"

      - name: Upload benchmark results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: benchmark-results
          path: benchmark-results/
          retention-days: 90  # Keep results for 90 days

      - name: Prepare GitHub Pages visualization
        run: |
          # Get current date for badge timestamp in Berlin time
          TIMESTAMP=$(TZ='Europe/Berlin' date +"%Y-%m-%d")
          TIMESTAMP_WITH_TIME=$(TZ='Europe/Berlin' date +"%Y-%m-%d %H:%M %Z")

          # Create header for badge markdown
          echo "## Benchmark Results ($TIMESTAMP)" > gh-pages/badge-markdown.txt

          # Process micro benchmarks (legacy jmh-result.json for compatibility)
          if [ -f "benchmark-results/micro-benchmark-result.json" ]; then
            echo "Processing micro benchmark results..."
            # Copy for legacy compatibility
            cp benchmark-results/micro-benchmark-result.json benchmark-results/jmh-result.json
            
            # Prepare GitHub Pages structure using script for micro benchmarks
            bash cui-jwt-benchmarking/doc/templates/scripts/prepare-github-pages.sh benchmark-results cui-jwt-benchmarking/doc/templates gh-pages

            # Create performance badge using script for micro benchmarks
            bash cui-jwt-benchmarking/doc/templates/scripts/create-performance-badge.sh gh-pages/jmh-result.json gh-pages/badges

            # Create simple badges using script for micro benchmarks
            bash cui-jwt-benchmarking/doc/templates/scripts/create-simple-badge.sh gh-pages/jmh-result.json gh-pages/badges "$TIMESTAMP" "$TIMESTAMP_WITH_TIME"

            # Performance Tracking System for micro benchmarks
            echo "Setting up micro benchmark performance tracking..."
            COMMIT_HASH="${{ github.sha }}"
            
            # Create performance tracking data using script
            TRACKING_OUTPUT=$(bash cui-jwt-benchmarking/doc/templates/scripts/create-performance-tracking.sh gh-pages/jmh-result.json cui-jwt-benchmarking/doc/templates gh-pages "$COMMIT_HASH" 2>&1)
            echo "$TRACKING_OUTPUT"
            
            # Extract metrics from tracking script output
            PERF_SCORE=$(echo "$TRACKING_OUTPUT" | grep "PERF_SCORE=" | cut -d'=' -f2)
            PERF_THROUGHPUT=$(echo "$TRACKING_OUTPUT" | grep "PERF_THROUGHPUT=" | cut -d'=' -f2)
            PERF_LATENCY=$(echo "$TRACKING_OUTPUT" | grep "PERF_LATENCY=" | cut -d'=' -f2)
            PERF_RESILIENCE=$(echo "$TRACKING_OUTPUT" | grep "PERF_RESILIENCE=" | cut -d'=' -f2)
            
            if [ -n "$PERF_SCORE" ] && [ "$PERF_SCORE" != "0" ]; then
              # Update consolidated tracking and create trend badge using script
              bash cui-jwt-benchmarking/doc/templates/scripts/update-performance-trends.sh cui-jwt-benchmarking/doc/templates gh-pages "$COMMIT_HASH" "$PERF_SCORE" "$PERF_THROUGHPUT" "$PERF_LATENCY" "$PERF_RESILIENCE"
            else
              echo "Warning: Could not extract valid performance metrics for trend tracking"
              echo "{\"schemaVersion\":1,\"label\":\"Performance Trend\",\"message\":\"â†’ No Data\",\"color\":\"lightgrey\"}" > "gh-pages/badges/trend-badge.json"
            fi
          else
            echo "Warning: micro-benchmark-result.json not found"
            echo "{\"schemaVersion\":1,\"label\":\"Performance Score\",\"message\":\"No Data\",\"color\":\"red\"}" > "gh-pages/badges/performance-badge.json"
            echo "{\"schemaVersion\":1,\"label\":\"Performance Trend\",\"message\":\"â†’ No Data\",\"color\":\"lightgrey\"}" > "gh-pages/badges/trend-badge.json"
          fi

          # Process integration benchmarks
          if [ -f "benchmark-results/integration-benchmark-result.json" ]; then
            echo "Processing integration benchmark results..."
            mkdir -p gh-pages/integration/badges
            
            # Copy integration results to dedicated location
            cp benchmark-results/integration-benchmark-result.json gh-pages/integration/jmh-result.json
            
            # Copy integration template
            cp cui-jwt-benchmarking/doc/templates/integration-index.html gh-pages/integration/index.html
            
            # Process integration results using dedicated script
            INTEGRATION_OUTPUT=$(bash cui-jwt-benchmarking/doc/templates/scripts/process-integration-results.sh benchmark-results/integration-benchmark-result.json gh-pages "$COMMIT_HASH" 2>&1)
            echo "$INTEGRATION_OUTPUT"
            
            # Extract integration metrics from script output
            INTEGRATION_SCORE=$(echo "$INTEGRATION_OUTPUT" | grep "INTEGRATION_SCORE=" | cut -d'=' -f2)
            INTEGRATION_THROUGHPUT=$(echo "$INTEGRATION_OUTPUT" | grep "INTEGRATION_THROUGHPUT=" | cut -d'=' -f2)
            INTEGRATION_LATENCY=$(echo "$INTEGRATION_OUTPUT" | grep "INTEGRATION_LATENCY=" | cut -d'=' -f2)
            
            echo "Integration Performance Metrics:"
            echo "  Score: $INTEGRATION_SCORE"
            echo "  Throughput: $INTEGRATION_THROUGHPUT ops/s"
            echo "  Latency: $INTEGRATION_LATENCY ms"
            
            # Create integration trend tracking (similar to micro benchmarks)
            if [ -n "$INTEGRATION_SCORE" ] && [ "$INTEGRATION_SCORE" != "0" ]; then
              echo "Setting up integration benchmark performance tracking..."
              
              # Create integration-specific tracking file
              INTEGRATION_TRACKING_FILE="gh-pages/integration/performance-tracking.json"
              curl -f -s "https://cuioss.github.io/cui-jwt/benchmarks/integration/performance-tracking.json" -o "$INTEGRATION_TRACKING_FILE" 2>/dev/null || echo '{"runs":[]}' > "$INTEGRATION_TRACKING_FILE"
              
              # Add current integration run to tracking
              INTEGRATION_RUN=$(cat <<EOF
          {
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "commit": "$COMMIT_HASH",
            "performance": {
              "score": $INTEGRATION_SCORE,
              "throughput": {"value": $INTEGRATION_THROUGHPUT, "unit": "ops/s"},
              "averageTime": {"value": $INTEGRATION_LATENCY, "unit": "ms"},
              "errorResilience": {"value": 100, "unit": "ops/s"}
            }
          }
          EOF
              )
              
              # Update integration tracking file
              jq --argjson newrun "$INTEGRATION_RUN" '.runs += [$newrun] | .runs = (.runs | sort_by(.timestamp) | .[-10:])' "$INTEGRATION_TRACKING_FILE" > "$INTEGRATION_TRACKING_FILE.tmp" && mv "$INTEGRATION_TRACKING_FILE.tmp" "$INTEGRATION_TRACKING_FILE"
              
              # Create integration trend badge
              bash cui-jwt-benchmarking/doc/templates/scripts/calculate-trend-badge.sh "$INTEGRATION_TRACKING_FILE" "gh-pages/integration/badges"
              
              # Copy performance trends template for integration
              cp cui-jwt-benchmarking/doc/templates/performance-trends.html gh-pages/integration/trends.html
            else
              echo "Warning: Could not extract valid integration performance metrics for trend tracking"
              echo "{\"schemaVersion\":1,\"label\":\"Integration Performance Trend\",\"message\":\"â†’ No Data\",\"color\":\"lightgrey\"}" > "gh-pages/integration/badges/trend-badge.json"
            fi
            
            # Add integration benchmark link to main page
            echo "- [Integration Benchmark Results](integration/index.html)" >> gh-pages/badge-markdown.txt
          else
            echo "Warning: integration-benchmark-result.json not found"
            mkdir -p gh-pages/integration/badges
            echo "{\"schemaVersion\":1,\"label\":\"Integration Performance\",\"message\":\"No Data\",\"color\":\"red\"}" > "gh-pages/integration/badges/performance-badge.json"
            echo "{\"schemaVersion\":1,\"label\":\"Integration Performance Trend\",\"message\":\"â†’ No Data\",\"color\":\"lightgrey\"}" > "gh-pages/integration/badges/trend-badge.json"
          fi

      - name: Deploy to cuioss.github.io
        uses: JamesIves/github-pages-deploy-action@6c2d9db40f9296374acc17b90404b6e8864128c8 # v4.7.3
        with:
          folder: gh-pages
          repository-name: cuioss/cuioss.github.io
          target-folder: cui-jwt/benchmarks
          branch: main
          token: ${{ secrets.PAGES_DEPLOY_TOKEN }}
